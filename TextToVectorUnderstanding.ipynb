{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c219e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to vectors from text\n",
    "\n",
    "## one hot encoding -easy to implement but sparse matrix(overfitting)\n",
    "## dis- 2 - No fixed size INput\n",
    "## no semantic meaning(which word is important/linking in words)\n",
    "##out of vocab- new words are not captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63696b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words\n",
    "##Lower case the words\n",
    "## remove stopwords\n",
    "## frequency of words and arrange in desc order\n",
    "## just take words with more frequency and they will form features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "291fab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary Bag of Words - 0s and 1s\n",
    "#BOW - counted updated based on frequency\n",
    "\n",
    "#Adv\n",
    "#simple and intuative\n",
    "# Fixed size input for ML algo\n",
    "\n",
    "\n",
    "#Disadv\n",
    "#sparse matrix - more words ,overfitting\n",
    "#order changes - meaning changes\n",
    "# Out of vocab words - these words are ignored\n",
    "#semantic meaning not getting captured\n",
    "\n",
    "\n",
    "# eg : The food is good\n",
    "# the food is not good.\n",
    "#sentences have very less difference but semantic meaninhg is totally different\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9dafab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF- IFD[Term Frequency - Inverse Document Frequency]\n",
    "\n",
    "# TF = No of rep of Words in sent/No of words in sent\n",
    "#IDF = No of sent /No of sent containing the word\n",
    "\n",
    "\n",
    "#TF- IFD = TF * IDF\n",
    "\n",
    "#Adv\n",
    "# simple and intuative\n",
    "# Fixed size input for ML algo\n",
    "# word imp captured - common words are less and specific words are captured\n",
    "\n",
    "#diadv\n",
    "#sparse matrix\n",
    "#Out of vocab words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78214a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding\n",
    "\n",
    "#representation of words for analysis- real valued vector which \n",
    "#encodes the meaning of word such that the words that are closer\n",
    "# in the vector space are expected to be similar in meaning\n",
    "\n",
    "#eg : Happy and Excited -similar\n",
    "#Happy and angry are far\n",
    "\n",
    "#types\n",
    "#1. Count or frequency (One hot encoding, BOW,TF-IFD)\n",
    "# 2. Deep learning Trained Model(Word2Vec)\n",
    "\n",
    "#Word2Vec\n",
    "#1. CBow - Continuous BAG of words\n",
    "#2. skipgram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d082ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec\n",
    "#Uses Neural Network - learn word association from large text\n",
    "#After train - detect synonymous words/suggest additional\n",
    "# words for a partial sentence\n",
    "\n",
    "#each word with list of numbers called vector\n",
    "#Features - represent on columns\n",
    "#Rows - distinct words\n",
    "#values - if relationship can be formed, value close to 1 else near to 0\n",
    "\n",
    "#cosine similarity - angle distance between to is less =0\n",
    "#if far then value far away\n",
    "\n",
    "#Depends on ANN,Loss function and Optimisers\n",
    "\n",
    "\n",
    "#in this\n",
    "#1. can take pretrained model\n",
    "#2. Train a model from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c66433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CBOW -corpus/dataset\n",
    "\n",
    "#window size \n",
    "#central word\n",
    "#input - before central word, after central word\n",
    "#output - central word\n",
    "\n",
    "# each word is represented by vector using one hot encoding\n",
    "\n",
    "#Fully connected NN in cbow\n",
    "# I/p layer - 4 words each of size as the vector in one hot encoding\n",
    "#eg  4 layers - 7 size vectors (each word represented using 7 size of vector)\n",
    "\n",
    "# Middle layer\n",
    "# window size is 5 so represented by 7 vectors\n",
    "# each number in 4 vectors will be connected to each number in vector\n",
    "\n",
    "\n",
    "#output\n",
    "#vector of 7 layer\n",
    "# y is the original value\n",
    "#y^ (yhat) is the predicted value we get\n",
    "# we calculate loss function and based on that we \n",
    "# do backward propogation till loss value is minimal\n",
    "\n",
    "\n",
    "#when window size -5 then output 5 vectors\n",
    "#feature representation of 5\n",
    "#more window size more better the model will perform\n",
    "# here the output layer is of size 7 vector and each vector here \n",
    "#has size 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c036fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skipgram\n",
    "\n",
    "# Input layer - vector size 7 - one vector (one word at a time)\n",
    "# Hidden. layer - 5 size vector\n",
    "# Output - 4 vectors of size 7 (like input in cbow)\n",
    "#y, yhat,loss ,backward prop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd8ae4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when to use\n",
    "# small dataset - cbow\n",
    "#huge dataset - skipgram\n",
    "\n",
    "#how to improve any\n",
    "#increase training data\n",
    "#increase window size -better model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da37b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google word2vec\n",
    "\n",
    "#feature representation of 300  dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f79ca049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg word2vec\n",
    "\n",
    "#use pretrained model word2vec\n",
    "#each word represented using 300 dims vector\n",
    "# aim is to get one dimension for each document /sentence\n",
    "#so we take avg of these dims in one dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c051a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gensim lib/Glove lib\n",
    "\n",
    "#---> pretrained model\n",
    "#---> train a model from scratch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
